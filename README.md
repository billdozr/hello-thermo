# THRML-Powered Restricted Boltzmann Machine

A Restricted Boltzmann Machine (RBM) implementation using the THRML library's hardware-efficient Gibbs sampling for the bars-and-stripes dataset.

## Overview

This project demonstrates training an RBM on the bars-and-stripes pattern recognition task using THRML's Ising model framework for efficient Gibbs sampling. The implementation showcases both THRML's accelerated sampling and includes a naive Python baseline for performance comparison.

## What is an RBM?

A Restricted Boltzmann Machine is an energy-based probabilistic graphical model with:
- **Visible layer**: Input data (64 pixels for 8×8 images)
- **Hidden layer**: Latent features (128 units)
- **Bipartite structure**: Connections only between layers, not within layers

The model learns to represent data patterns through an energy function formulated as an Ising model:
```
E(v,h) = -β(Σ b_i s_i + Σ J_ij s_i s_j)
```

## Architecture

### Model Configuration
- **Visible units**: 64 (8×8 pixel grid)
- **Hidden units**: 128
- **Training data**: 256 bars-and-stripes patterns (subsampled from 508 total)
- **Inverse temperature (β)**: 1.0
- **Learning rate**: 0.05
- **Training epochs**: 200

### Training Strategy

**Positive Phase** (data-driven):
- Visible units clamped to data
- 16 parallel chains sampling hidden units
- Schedule: 20 warmup steps, 20 samples, 2 steps between samples

**Negative Phase** (model-driven):
- Both visible and hidden units free
- 256 parallel fantasy chains
- Schedule: 50 warmup steps, 20 samples, 5 steps between samples

**Gradient Estimation**:
Uses Contrastive Divergence via THRML's `estimate_kl_grad()` to compute KL divergence gradients with respect to biases and weights.

## Key Components

### 1. Data Generation ([rbm.py:21-44](rbm.py#L21-L44))
`make_bars_stripes()` generates binary patterns with horizontal/vertical bars:
- Each pattern is a subset of rows (horizontal) or columns (vertical) turned ON
- Excludes all-black and all-white patterns
- Returns 8×8 flattened boolean arrays

### 2. THRML Integration ([rbm.py:83-106](rbm.py#L83-L106))
Constructs an `IsingEBM` model:
- Creates `SpinNode` objects for visible/hidden units
- Defines fully-connected bipartite edges
- Initializes small random biases and weights

### 3. Conditional Sampling ([rbm.py:159-186](rbm.py#L159-L186))
Implements exact Ising conditionals:
- `sample_hidden_given_visible()`: P(h|v) using field h_j = b_j + Σ_i J_ij s_i
- `sample_visible_given_hidden()`: P(v|h) using field v_i = b_i + Σ_j J_ij s_j
- Bernoulli sampling with probability σ(2βh) where σ is sigmoid

### 4. Training Loop ([rbm.py:311-373](rbm.py#L311-L373))
For each epoch:
1. Initialize positive/negative chain states with `hinton_init()`
2. Estimate gradients via `estimate_kl_grad()` using data and model samples
3. Gradient descent: update biases and weights
4. Track reconstruction error (MSE, BCE) and weight norm

### 5. Free-Running Sampling ([rbm.py:472-563](rbm.py#L472-L563))
After training, generates unconditioned samples:
- Uses THRML's `sample_states()` with both layers free
- Schedule: 300 warmup, 16 samples, 20 steps between samples
- Demonstrates what patterns the model has learned

### 6. Python Baseline ([rbm.py:241-298](rbm.py#L241-L298))
`gibbs_python_baseline()` implements naive block Gibbs in NumPy:
- Manual alternating updates of hidden then visible layers
- Same sampling schedule as THRML for fair comparison
- Pure Python loops (no JIT compilation)

## Outputs

The script generates six visualizations:

1. **01_dataset_samples.png** - Sample bars-and-stripes training data
2. **02_training_curves.png** - MSE, BCE, and weight norm over 200 epochs
3. **03_hidden_filters.png** - 128 learned feature detectors (weights reshaped to 8×8)
4. **04_reconstructions.png** - Original vs one-step reconstructed images
5. **05_free_running_samples.png** - 16 patterns generated by THRML sampler
6. **06_free_running_python_samples.png** - 16 patterns from Python baseline

## Performance Comparison

The implementation includes timing benchmarks comparing THRML's hardware-optimized Gibbs sampling against a naive Python implementation. THRML leverages JAX's JIT compilation and XLA optimization for significant speedup.

Expected output:
```
THRML free-running sampling (no compile) elapsed: X.XXXX s
Naive Python Gibbs sampling elapsed: Y.YYYY s
Speed ratio (Python / THRML): ZZ.ZZx slower
```

## How It Works

### Energy-Based Learning

The RBM learns by adjusting its energy landscape to:
- **Lower energy** for observed data patterns (positive phase)
- **Raise energy** for model-generated patterns (negative phase)

This is achieved through gradient descent on the KL divergence between data and model distributions.

### Ising Model Formulation

THRML represents the RBM as an Ising spin glass:
- Boolean pixels {0,1} map to spins {-1,+1}
- Biases become local fields
- Weights become coupling constants
- Exact conditional distributions enable efficient block Gibbs sampling

### Block Gibbs Sampling

Since layers are conditionally independent given the other layer:
- All hidden units can be sampled in parallel given visible states
- All visible units can be sampled in parallel given hidden states
- This bipartite structure enables efficient hardware parallelization

## Running the Code

```bash
python rbm.py
```

This will:
1. Generate the bars-and-stripes dataset
2. Train the RBM for 200 epochs (~2-5 minutes depending on hardware)
3. Generate all visualization plots
4. Print training progress and performance metrics
5. Save outputs to current directory

## Requirements

- JAX
- NumPy
- Matplotlib
- THRML library

## Technical Details

- **Initialization**: Small random weights/biases (0.01 × N(0,1))
- **Vectorization**: Uses `jax.vmap` for parallel reconstruction error computation
- **JIT compilation**: THRML automatically JIT-compiles sampling kernels
- **Memory efficiency**: Block structure avoids explicit full-graph representations
